{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the folders:  800\n",
      "Length of the pIDs:  800\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Step 3\n",
    "mainFolderPath = './Data-Inference/'\n",
    "modelPath = r'D:\\Teknofest\\Inference\\Inference\\4x-ConvNext-SWIN-E1-448.pt'\n",
    "\n",
    "#Read Folders\n",
    "folderPaths = [mainFolderPath + f for f in os.listdir(mainFolderPath)]\n",
    "pIDs = [str(f)  for f in os.listdir(mainFolderPath)]\n",
    "\n",
    "print(\"Length of the folders: \", len(folderPaths))\n",
    "print(\"Length of the pIDs: \", len(pIDs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define The Object\n",
    "JSONoutputs = {\n",
    "    \"kunye\": {\n",
    "        \"takim_adi\": \"PIXELERS\",\n",
    "        \"takim_id\": \"539822\",\n",
    "        \"aciklama\": \"\",\n",
    "        \"versiyon\": \"0.0\"\n",
    "    },\n",
    "    \"tahminler\": {\n",
    "        \n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ConvNext(nn.Module):\n",
    "\tdef __init__(self, num_classes=4):\n",
    "\t\tsuper(ConvNext, self).__init__()\n",
    "\t\t\n",
    "\t\tself.convnext = models.convnext_small(pretrained=False)\n",
    "\t\tself.convnext2 = models.convnext_small(pretrained=False)\n",
    "\t\tself.convnext3 = models.convnext_small(pretrained=False)\n",
    "\t\tself.convnext4 = models.convnext_small(pretrained=False)\n",
    "\n",
    "\t\t# Replace the classifiers with identities to extract features\n",
    "\t\tself.convnext.classifier[2] = nn.Identity()\n",
    "\t\tself.convnext2.classifier[2] = nn.Identity()\n",
    "\t\tself.convnext3.classifier[2] = nn.Identity()\n",
    "\t\tself.convnext4.classifier[2] = nn.Identity()\n",
    "\t\t\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\tnn.Linear(3072  , 512),\n",
    "\t\t\tnn.Dropout(0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, num_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, image,image2):\n",
    "\t\tB, C, H, W = image.size()\n",
    "\t\t\n",
    "\t\tif H % 2 != 0 or W % 2 != 0:\n",
    "\t\t\traise ValueError(\"Image height and width must be divisible by 2.\")\n",
    "\t\t\n",
    "\t\ttop_left = image[:, :, :H//2, :W//2]\n",
    "\t\ttop_right = image[:, :, :H//2, W//2:]\n",
    "\t\tbottom_left = image[:, :, H//2:, :W//2]\n",
    "\t\tbottom_right = image[:, :, H//2:, W//2:]\n",
    "\t\t\n",
    "\t\tfeatures1 = self.convnext(top_left)\n",
    "\t\tfeatures2 = self.convnext2(top_right)\n",
    "\t\tfeatures3 = self.convnext3(bottom_left)\n",
    "\t\tfeatures4 = self.convnext4(bottom_right)\n",
    "\t\t\n",
    "\t\tcombined_features = torch.cat((features1, features2, features3, features4), dim=1)\n",
    "\t\t\n",
    "\t\toutput = self.fc(combined_features.view(combined_features.size(0), -1))\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "\n",
    "class SWIN(nn.Module):\n",
    "\tdef __init__(self, num_classes=4):\n",
    "\t\tsuper(SWIN, self).__init__()\n",
    "\t\t\n",
    "\t\tself.swin = models.swin_b(pretrained=False)\n",
    "\t\tself.swin2 = models.swin_b(pretrained=False)\n",
    "\t\tself.swin3 = models.swin_b(pretrained=False)\n",
    "\t\tself.swin4 = models.swin_b(pretrained=False)\n",
    "\n",
    "\t\t# Replace the classifiers with identities to extract features\n",
    "\t\tself.swin.head = nn.Identity()\n",
    "\t\tself.swin2.head = nn.Identity()\n",
    "\t\tself.swin3.head = nn.Identity()\n",
    "\t\tself.swin4.head = nn.Identity()\n",
    "\t\t\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\tnn.Linear(4096  , 512),\n",
    "\t\t\tnn.Dropout(0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, num_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, image,image2):\n",
    "\t\tB, C, H, W = image.size()\n",
    "\t\t\n",
    "\t\tif H % 2 != 0 or W % 2 != 0:\n",
    "\t\t\traise ValueError(\"Image height and width must be divisible by 2.\")\n",
    "\t\t\n",
    "\t\ttop_left = image[:, :, :H//2, :W//2]\n",
    "\t\ttop_right = image[:, :, :H//2, W//2:]\n",
    "\t\tbottom_left = image[:, :, H//2:, :W//2]\n",
    "\t\tbottom_right = image[:, :, H//2:, W//2:]\n",
    "\t\t\n",
    "\t\tfeatures1 = self.swin(top_left)\n",
    "\t\tfeatures2 = self.swin2(top_right)\n",
    "\t\tfeatures3 = self.swin3(bottom_left)\n",
    "\t\tfeatures4 = self.swin4(bottom_right)\n",
    "\t\t\n",
    "\t\tcombined_features = torch.cat((features1, features2, features3, features4), dim=1)\n",
    "\t\t\n",
    "\t\toutput = self.fc(combined_features.view(combined_features.size(0), -1))\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\t\n",
    " \n",
    "\t\n",
    "class CombinedModel(nn.Module):\n",
    "\tdef __init__(self, num_classes=4):\n",
    "\t\tsuper(CombinedModel, self).__init__()\n",
    "\t\t\n",
    "\t\tself.convNext = ConvNext()\n",
    "\t\tself.swin = SWIN()\n",
    "\t\t# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\t\t\n",
    "\t\t# self.convNext.load_state_dict(torch.load('./Saved/4x-ConvNext-E10-448.pt', map_location=device))\n",
    "\t\t# self.swin.load_state_dict(torch.load('./Saved/4x-SWIN-E16-448.pt', map_location=device))\n",
    "\t\t\n",
    "\t\t# Replace the classifiers with identities to extract features\n",
    "\t\tself.convNext.fc = nn.Identity()\n",
    "\t\tself.swin.fc = nn.Identity()\n",
    "\n",
    "\n",
    "\t\tself.fc = nn.Sequential(\n",
    "            nn.Linear(7168, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "\tdef forward(self, image1):\n",
    "\t\tfeatures1 = self.convNext(image1, '')\n",
    "\t\tfeatures3 = self.swin(image1, '')\n",
    "\t\tcombined_features = torch.cat((features1,features3), dim=1)\t\t\n",
    "\t\t\n",
    "\t\toutput = self.fc(combined_features)\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((448, 448)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "\tdef __init__(self, transform=None):\n",
    "\t\tself.transform = transform\n",
    "\t\tself.image_data = []\n",
    "\n",
    "\t\tfor i in tqdm(range(len(folderPaths))):\n",
    "\t\t\timage_dir = folderPaths[i]\n",
    "\t\t\tpID = pIDs[i]\n",
    "\n",
    "\t\t\tfiles = os.listdir(image_dir)\n",
    "\t\t\tif 'CC' in files[0]:\n",
    "\t\t\t\tfile1 = os.path.join(image_dir, files[0])\n",
    "\t\t\t\tfile2 = os.path.join(image_dir, files[1])\n",
    "\t\t\telse:\n",
    "\t\t\t\tfile1 = os.path.join(image_dir, files[1])\n",
    "\t\t\t\tfile2 = os.path.join(image_dir, files[0])\n",
    "\n",
    "\t\t\tself.image_data.append((file1,file2, pID))\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.image_data)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tfile1, file2, pID = self.image_data[idx]\n",
    "\t\t\n",
    "\t\timage1 = cv.imread(file1)\n",
    "\t\timage2 = cv.imread(file2)\n",
    "\n",
    "\t\t\n",
    "\t\tcombined_image = cv.hconcat([image1, image2])\n",
    "\t\t\n",
    "\t\tcombined_image = Image.fromarray(combined_image)\n",
    "\t\tif self.transform:\n",
    "\t\t\tnewImage = self.transform(combined_image)\n",
    "\n",
    "\t\treturn newImage, pID\n",
    "\t\n",
    "def get_batches(dataset, batch_size, shuffle=False):\n",
    "\tindices = np.arange(len(dataset))\n",
    "\tfor start_idx in range(0, len(dataset), batch_size):\n",
    "\t\tbatch_indices = indices[start_idx:start_idx + batch_size]\n",
    "\t\tbatch = [dataset[idx] for idx in batch_indices]\n",
    "\t\timages, pIDS = zip(*batch)\n",
    "\t\timages = torch.stack(images)\n",
    "\t\tyield images, pIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 18172.70it/s]\n",
      "c:\\Users\\ihirc\\anaconda3\\envs\\U_Net\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ihirc\\anaconda3\\envs\\U_Net\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "8\n",
      "16\n",
      "24\n",
      "32\n",
      "40\n",
      "48\n",
      "56\n",
      "64\n",
      "72\n",
      "80\n",
      "88\n",
      "96\n",
      "104\n",
      "112\n",
      "120\n",
      "128\n",
      "136\n",
      "144\n",
      "152\n",
      "160\n",
      "168\n",
      "176\n",
      "184\n",
      "192\n",
      "200\n",
      "208\n",
      "216\n",
      "224\n",
      "232\n",
      "240\n",
      "248\n",
      "256\n",
      "264\n",
      "272\n",
      "280\n",
      "288\n",
      "296\n",
      "304\n",
      "312\n",
      "320\n",
      "328\n",
      "336\n",
      "344\n",
      "352\n",
      "360\n",
      "368\n",
      "376\n",
      "384\n",
      "392\n",
      "400\n",
      "408\n",
      "416\n",
      "424\n",
      "432\n",
      "440\n",
      "448\n",
      "456\n",
      "464\n",
      "472\n",
      "480\n",
      "488\n",
      "496\n",
      "504\n",
      "512\n",
      "520\n",
      "528\n",
      "536\n",
      "544\n",
      "552\n",
      "560\n",
      "568\n",
      "576\n",
      "584\n",
      "592\n",
      "600\n",
      "608\n",
      "616\n",
      "624\n",
      "632\n",
      "640\n",
      "648\n",
      "656\n",
      "664\n",
      "672\n",
      "680\n",
      "688\n",
      "696\n",
      "704\n",
      "712\n",
      "720\n",
      "728\n",
      "736\n",
      "744\n",
      "752\n",
      "760\n",
      "768\n",
      "776\n",
      "784\n",
      "792\n",
      "800\n",
      "Length of the predictions:  800\n",
      "Length of the pIDs:  800\n"
     ]
    }
   ],
   "source": [
    "data = CustomImageDataset(transform=transform)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = CombinedModel(num_classes=4)\n",
    "model.to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(modelPath, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_pIDS = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor images, pIDS in get_batches(data, 8):\n",
    "\t\timages = images.to(device)\n",
    "\t\toutputs = model(images)\n",
    "\t\t_, preds = torch.max(outputs, 1)\n",
    "\t\tall_preds.extend(preds.cpu().numpy())\n",
    "\t\tall_pIDS.extend(pIDS)\n",
    "\t\tprint(len(all_preds))\n",
    "\n",
    "print(\"Length of the predictions: \", len(all_preds))\n",
    "print(\"Length of the pIDs: \", len(all_pIDS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_pIDS)):\n",
    "\tcat = '1'\n",
    "\tif (all_preds[i] == 1):\n",
    "\t\tcat = '2'\n",
    "\telif (all_preds[i] == 2):\n",
    "\t\tcat = '4'\n",
    "\telif (all_preds[i] == 3):\n",
    "\t\tcat = '5'\n",
    "\tJSONoutputs['tahminler'][all_pIDS[i]] = {\n",
    "\t\t\"kategori\" : cat\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('outputs.json', 'w') as f:\n",
    "\tjson.dump(JSONoutputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
